{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "MAE: 11.6833\n",
      "MSE: 234.7637\n",
      "RMSE: 15.3220\n",
      "R²: 0.5777\n",
      "Note: These metrics are based on a single test split. Performance may vary with different datasets or cross-validation strategies.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io  # Direct import for loading .mat files\n",
    "import scipy.signal  # Direct import for signal processing functions\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Load dataset file paths for Troika dataset.\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Load data from a .mat file.\n",
    "    \"\"\"\n",
    "    # Load the .mat file\n",
    "    data = scipy.io.loadmat(data_fl)\n",
    "    \n",
    "    # Ensure 'sig' key exists in the loaded data\n",
    "    if 'sig' not in data:\n",
    "        raise KeyError(f\"'sig' key not found in {data_fl}\")\n",
    "    \n",
    "    # Return all data except the first two elements\n",
    "    return data['sig'][2:]\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Compute the aggregate error metric based on the 90th percentile confidence.\n",
    "    \"\"\"\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "fs = 125  # signals were sampled at 125 Hz\n",
    "minBPM = 40  # min bpm\n",
    "maxBPM = 240  # max bpm\n",
    "window_length = 8 * fs  # 8 second time window\n",
    "window_shift = 2 * fs  # 2 second shift to next window\n",
    "\n",
    "def bandpass_filter(signal, fs):\n",
    "    \"\"\"\n",
    "    Apply a bandpass filter to the signal.\n",
    "    \"\"\"\n",
    "    pass_band = (minBPM/60, maxBPM/60)\n",
    "    b, a = scipy.signal.cheby2(4, 20, pass_band, btype='bandpass', fs=fs)\n",
    "    return scipy.signal.filtfilt(b, a, signal)\n",
    "\n",
    "def fourier_transform(signal, fs):\n",
    "    \"\"\"\n",
    "    Compute the Fourier Transform of the signal.\n",
    "    \"\"\"\n",
    "    fft = np.abs(np.fft.rfft(signal))\n",
    "    freqs = np.fft.rfftfreq(len(signal), 1/fs)\n",
    "    return fft, freqs\n",
    "\n",
    "def calculate_confidence(freqs, fft_f, bpm_max):\n",
    "    \"\"\"\n",
    "    Calculate confidence based on the FFT within a specific frequency window.\n",
    "    \"\"\"\n",
    "    fundamental_freq_window = (freqs > bpm_max - minBPM/60) & (freqs < bpm_max + minBPM/60)\n",
    "    return np.sum(fft_f[fundamental_freq_window]) / np.sum(fft_f) if np.sum(fft_f) != 0 else 0\n",
    "\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "    ground_truth = sp.io.loadmat(ref_fl)['BPM0'].flatten()  # Ensure ground_truth is a 1D array\n",
    "\n",
    "    # Apply bandpass filter to signals\n",
    "    ppg = bandpass_filter(ppg, fs)\n",
    "    accx = bandpass_filter(accx, fs)\n",
    "    accy = bandpass_filter(accy, fs)\n",
    "    accz = bandpass_filter(accz, fs)\n",
    "\n",
    "    bpm_pred = []\n",
    "    confidence = []\n",
    "    features = []\n",
    "\n",
    "    # Ensure that the length of ground_truth is sufficient for the windows we process\n",
    "    if len(ppg) < window_length:\n",
    "        raise ValueError(\"PPG signal is too short for the specified window length\")\n",
    "\n",
    "    for i in range(0, len(ppg) - window_length + 1, window_shift):\n",
    "        ppg_window = ppg[i:i+window_length]\n",
    "        acc_window = np.sqrt(accx[i:i+window_length]**2 + accy[i:i+window_length]**2 + accz[i:i+window_length]**2)\n",
    "\n",
    "        fft_ppg, ppg_freqs = fourier_transform(ppg_window, fs)\n",
    "        fft_acc, acc_freqs = fourier_transform(acc_window, fs)\n",
    "\n",
    "        fft_ppg[ppg_freqs <= (minBPM)/60.0] = 0.0\n",
    "        fft_ppg[ppg_freqs >= (maxBPM)/60.0] = 0.0\n",
    "        fft_acc[acc_freqs <= (minBPM)/60.0] = 0.0\n",
    "        fft_acc[acc_freqs >= (maxBPM)/60.0] = 0.0\n",
    "\n",
    "        ppg_max = ppg_freqs[np.argmax(fft_ppg)]\n",
    "        acc_max = acc_freqs[np.argmax(fft_acc)]\n",
    "\n",
    "        combined_max = (ppg_max + acc_max) / 2\n",
    "        conf_val = calculate_confidence(ppg_freqs, fft_ppg, ppg_max)\n",
    "\n",
    "        bpm_pred.append(ppg_max * 60)\n",
    "        confidence.append(conf_val)\n",
    "\n",
    "        feature = [ppg_max * 60, acc_max * 60, combined_max * 60, conf_val]\n",
    "        features.append(feature)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    features = np.array(features)\n",
    "    bpm_pred = np.array(bpm_pred)\n",
    "\n",
    "    # Match the length of ground_truth with bpm_pred\n",
    "    if len(ground_truth) > len(bpm_pred):\n",
    "        ground_truth = ground_truth[:len(bpm_pred)]\n",
    "    elif len(bpm_pred) > len(ground_truth):\n",
    "        bpm_pred = bpm_pred[:len(ground_truth)]\n",
    "\n",
    "    errors = np.abs(np.subtract(ground_truth, bpm_pred))\n",
    "    return errors, confidence, features\n",
    "\n",
    "def TrainModel(features, bpm_pred):\n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0)\n",
    "    model.fit(features, bpm_pred)\n",
    "    return model\n",
    "\n",
    "\n",
    "def Evaluate():\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    all_features = []\n",
    "    all_bpm_preds = []\n",
    "    \n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        errors, confidence, features = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        all_features.append(features)\n",
    "        all_bpm_preds.extend(errors)\n",
    "\n",
    "    # Ensure that all_features and all_bpm_preds have the same length\n",
    "    all_features = np.vstack(all_features)\n",
    "    all_bpm_preds = np.array(all_bpm_preds)\n",
    "\n",
    "    if len(all_features) != len(all_bpm_preds):\n",
    "        raise ValueError(f\"Feature and target lengths do not match: {len(all_features)} vs {len(all_bpm_preds)}\")\n",
    "\n",
    "    X = all_features\n",
    "    y = all_bpm_preds\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    model = TrainModel(X_train, y_train)\n",
    "    predicted_bpm = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, predicted_bpm)\n",
    "    mse = mean_squared_error(y_test, predicted_bpm)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, predicted_bpm)\n",
    "\n",
    "    print(f\"Evaluation Metrics:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "    # Additional comment about generalizability\n",
    "    print(\"Note: These metrics are based on a single test split. Performance may vary with different datasets or cross-validation strategies.\")\n",
    "\n",
    "Evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "> - **Code Description** - Include details so someone unfamiliar with your project will know how to run your code and use your algorithm. \n",
    "###### Data Loading: The LoadTroikaDataset function retrieves data file paths and reference files from a specified directory. \n",
    "###### Preprocessing: Signals are filtered using a bandpass filter to isolate the frequency range corresponding to the heart rate (BPM). \n",
    "###### Feature Extraction: The RunPulseRateAlgorithm function extracts features from the filtered signals, performs a Fourier transform to identify the frequency components, and calculates a confidence score for each window of data.\n",
    "###### Model Training and Evaluation: The TrainModel function initializes a Gradient Boosting Regressor model, which is then trained using features extracted from the signals. The Evaluate function splits the data into training and test sets, evaluates the model using error metrics, and prints the results.\n",
    "\n",
    "> - **Data Description** - Describe the dataset that was used to train and test the algorithm. Include its short-comings and what data would be required to build a more complete dataset.\n",
    "The dataset used for training and testing consists of physiological signals (PPG and accelerometer data) from the Troika dataset. The dataset includes:\n",
    "###### PPG Signals: Photoplethysmogram signals capturing blood volume changes. Accelerometer Signals: Signals from three axes of an accelerometer. \n",
    "###### Shortcomings: Limited Diversity: The dataset may not represent all possible variations in heart rate or signal quality across different populations. Noise and Artifacts: The signals might contain noise or artifacts that can impact model performance.\n",
    "###### Improvements: Expanded Dataset: Include data from diverse populations and different conditions. Enhanced Signal Quality: Use data with reduced noise and artifacts for better model training.\n",
    "\n",
    "> - **Algorithhm Description** will include the following:\n",
    ">   - how the algorithm works\n",
    ">   - the specific aspects of the physiology that it takes advantage of\n",
    ">   - a describtion of the algorithm outputs\n",
    ">   - caveats on algorithm outputs \n",
    ">   - common failure modes\n",
    "How the Algorithm Works:\n",
    "###### Signal Filtering: The algorithm applies a bandpass filter to isolate frequencies corresponding to the heart rate. Feature Extraction: It performs Fourier transforms to extract frequency components and calculates a confidence score based on the prominence of the fundamental frequency. Model Training: Uses a Gradient Boosting Regressor to predict the heart rate from the extracted features.\n",
    "###### Physiological Aspects: PPG Signal: Utilizes frequency information from the PPG signal to estimate the heart rate. Accelerometer Data: Combines accelerometer data to enhance the robustness of the estimation.\n",
    "###### Algorithm Outputs: Predicted BPM: The estimated heart rate in beats per minute. Confidence Scores: A measure of confidence in the predicted BPM based on the frequency components. \n",
    "###### Caveats on Algorithm Outputs: Accuracy Variability: Performance can vary based on signal quality and individual differences. Sensitivity to Noise: The algorithm may be sensitive to noise or artifacts in the signal.\n",
    "###### Common Failure Modes: Poor Signal Quality: Low-quality or noisy signals can lead to inaccurate BPM estimates. Inadequate Feature Extraction: If key features are not effectively captured, prediction accuracy may decrease. Overfitting: The model may overfit to the training data if not properly validated.\n",
    "\n",
    "> - **Algorithm Performance** - Detail how performance was computed (eg. using cross-validation or train-test split) and what metrics were optimized for. Include error metrics that would be relevant to users of your algorithm. Caveat your performance numbers by acknowledging how generalizable they may or may not be on different datasets.\n",
    "Performance Computation:\n",
    "###### Training and Testing: The dataset is split into training (80%) and test (20%) sets using train_test_split. The model is trained on the training set and evaluated on the test set. Metrics: The performance is measured using Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (R²).\n",
    "###### Generalizability Caveat: Performance Variability: The metrics reported are based on a single train-test split. Performance may vary with different datasets or when using cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
